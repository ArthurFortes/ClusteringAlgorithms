"""

PaCo Algorithm
Co-Clustering Algorithm

Paper: Improving Co-Cluster Quality with Application to Product Recommendations
Doi: http://dl.acm.org/citation.cfm?id=2661980
Authors: Michail Vlachos, Francesco Fusco, Charalambos Mavroforakis, Anastasios Kyrillidis, a
nd Vassilios G. Vassiliadis.
2014

"""

from caserec.utils.read_file import ReadFile
from scipy.spatial.distance import squareform, pdist
from scipy.stats import zscore
from sklearn.cluster import KMeans
from collections import defaultdict
import numpy as np
import itertools

__author__ = 'Arthur Fortes'


def normalize(v):
    norm = np.linalg.norm(v, ord=1)
    if norm == 0:
        norm = np.finfo(v.dtype).eps
    return v/norm


class PaCo(object):
    def __init__(self, training_file, prediction_file=None, k_row=5, l_col=5, number_pred_items=75, density_low=0.008):
        """
        :param training_file: (string:: file)
        :param k_row: (int) number of clusters generated by k-means in rows
        :param l_col: (int) number of clusters generated by k-means in rows
        :param density_low: (float) threshold to change the density matrix values
        """

        self.training_set = ReadFile(training_file).return_information(implicit=True)
        self.prediction_file = prediction_file
        self.k_row = k_row
        self.l_col = l_col
        self.number_pred_items = number_pred_items
        self.density_low = density_low

        self.list_row = [list() for _ in range(self.k_row)]
        self.list_col = [list() for _ in range(self.l_col)]

        self.count_total, self.count_ones = list(), list()
        self.density = None
        self.similarity_user = None
        self.similarity_item = None
        self.k_neighbors = int(np.sqrt(len(self.training_set['users'])))
        self.delta_entropy = list()
        self.uns_items = defaultdict()
        self.predictions = []

        np.random.seed(123)

    def run_kmeans(self):
        # Call kmeans++ to rows and cols
        clusters_rows = KMeans(n_clusters=self.k_row, init='k-means++').fit(self.training_set['matrix'])
        clusters_cols = KMeans(n_clusters=self.l_col, init='k-means++').fit(self.training_set['matrix'].T)

        # Map inverse index
        [self.list_row[label].append(row_id) for row_id, label in enumerate(clusters_rows.labels_)]
        [self.list_col[label].append(col_id) for col_id, label in enumerate(clusters_cols.labels_)]

    def count_information(self):
        for label_row in range(self.k_row):
            for label_col in range(self.l_col):
                count_local = 0

                for pair in itertools.product(self.list_row[label_row], self.list_col[label_col]):
                    if self.training_set['matrix'][pair[0]][pair[1]] != 0:
                        count_local += 1

                self.count_total.append(len(self.list_row[label_row]) * len(self.list_col[label_col]))
                self.count_ones.append(count_local)

        self.update_information(first_iteration=True)

    def update_information(self, first_iteration=False):
        """
        :param first_iteration: (bool) if True calculate self.count_total and self.count_ones
        """

        if first_iteration:
            self.count_total = np.matrix(self.count_total).reshape((self.k_row, self.l_col))
            self.count_ones = np.matrix(self.count_ones).reshape((self.k_row, self.l_col))
            self.density = np.matrix(np.divide(self.count_ones, self.count_total))
            # self.density = np.matrix(np.divide(self.count_ones, self.count_total)).reshape((self.k_row, self.l_col))
        else:
            self.density = np.matrix(np.divide(self.count_ones, self.count_total))
            self.density[self.density < self.density_low] = .0

    def calculate_entropy(self):
        total_density = self.density.sum()
        probability = np.divide(self.density, total_density)

        sum_pi = 0
        for pi in probability.flat:
            sum_pi += 0 if pi == 0 else pi * np.log2(pi)

        return (-sum_pi) / np.log2(probability.size)

    @staticmethod
    def return_min_value(matrix):
        min_value = (float('inf'), (0, 0))
        for i in range(len(matrix)):
            for j in range(i):
                if matrix[i][j] < min_value[0]:
                    min_value = (matrix[i][j], (i, j))

        return min_value

    def merge(self, min_value_row, min_value_col):

        if min_value_row[0] > min_value_col[0]:

            # merge of columns
            pair = min_value_col[1]

            new_set_col = self.list_col[pair[0]].copy() + self.list_col[pair[1]].copy()
            self.list_col = list(np.delete(self.list_col, [pair[0], pair[1]], axis=0))
            self.list_col.append(new_set_col)

            # update count total based on columns
            new_count_total = self.count_total[:, pair[0]] + self.count_total[:, pair[1]]
            self.count_total = np.delete(self.count_total, (pair[0], pair[1]), axis=1)
            self.count_total = np.insert(self.count_total, self.count_total.shape[1], new_count_total.T, axis=1)

            # update count ones based on columns
            new_count_ones = self.count_ones[:, pair[0]] + self.count_ones[:, pair[1]]
            self.count_ones = np.delete(self.count_ones, (pair[0], pair[1]), axis=1)
            self.count_ones = np.insert(self.count_ones, self.count_ones.shape[1], new_count_ones.T, axis=1)

        else:
            # merge of rows
            pair = min_value_row[1]

            new_set_row = self.list_row[pair[0]].copy() + self.list_row[pair[1]].copy()
            self.list_row = list(np.delete(self.list_row, [pair[0], pair[1]], axis=0))
            self.list_row.append(new_set_row)

            # update count total based on rows
            new_count_total = self.count_total[pair[0], :] + self.count_total[pair[1], :]
            self.count_total = np.delete(self.count_total, (pair[0], pair[1]), axis=0)
            self.count_total = np.insert(self.count_total, self.count_total.shape[0], new_count_total, axis=0)

            # update count ones based on rows
            new_count_ones = self.count_ones[pair[0], :] + self.count_ones[pair[1], :]
            self.count_ones = np.delete(self.count_ones, (pair[0], pair[1]), axis=0)
            self.count_ones = np.insert(self.count_ones, self.count_ones.shape[0], new_count_ones, axis=0)

        self.update_information()

    def train_model(self):
        count_epoch = 0
        criteria = True
        # 1st step: run k-means
        self.run_kmeans()
        # 2st step: collect information (only one time)
        self.count_information()

        entropy0 = self.calculate_entropy()

        # 3st step: training the algorithm
        while criteria:
            old_density, old_list_row, old_list_col = self.density.copy(), self.list_row.copy(), self.list_col.copy()
            distance_rows = np.divide(np.float32(squareform(pdist(self.density, 'euclidean'))), self.density.shape[1])
            distance_cols = np.divide(np.float32(squareform(pdist(self.density.T, 'euclidean'))), self.density.shape[0])
            min_row = self.return_min_value(distance_rows)
            min_col = self.return_min_value(distance_cols)

            self.merge(min_row, min_col)

            # Check the number os bi-clusters
            if len(self.list_row) == 1 and len(self.list_col) == 1:
                break

            entropy = self.calculate_entropy()
            dif_entropy = entropy - entropy0
            self.delta_entropy.append(dif_entropy)
            mean_range, std_range = np.mean(self.delta_entropy), np.std(self.delta_entropy)

            if not (mean_range - 3 * std_range <= dif_entropy <= mean_range + 3 * std_range):
                self.density, self.list_row, self.list_col = old_density, old_list_row, old_list_col
                criteria = False
            else:
                # print('Epoch:: ', count_epoch, " | Entropy:: ", entropy)
                entropy0 = entropy
                count_epoch += 1

        return entropy0

    # filters the bi-groups removing the ones with lower density,  leaving the minimum to recommend to every user
    def filter_relevant_bi_groups(self):
        filtered_densities = self.density.copy()
        filtered_densities[filtered_densities == 1] = np.nan

        first_run = True
        old = filtered_densities.copy()
        while True:
            for line in filtered_densities:
                print(np.nansum(line))
                if np.nansum(line) == 0:
                    if first_run:
                        return np.logical_not(np.isnan(self.density))
                    else:
                        return np.logical_not(np.isnan(old))

            old = filtered_densities.copy()

            # replace min in filteredDensities value per nan
            index = np.nanargmin(filtered_densities)
            filtered_densities.flat[index] = np.nan

            first_run = False

    def predict_per_user(self, user):
        u = self.training_set['mu'][user]
        rank = []
        score_u = []

        for item_j in self.uns_items[user]:
            users_ids = [self.training_set['mu'][user_j] for user_j in self.training_set['di'][item_j]]
            sim_sum = sorted(np.take(self.similarity_user[u], users_ids), key=lambda x: x)
            rank.append([user, item_j, sum(sim_sum[:self.k_neighbors])])
            score_u.append(sum(sim_sum[:self.k_neighbors]))

        score_u = np.array(score_u)
        score_u = (score_u - min(score_u)) / (max(score_u) - min(score_u))

        for s, sc in enumerate(score_u):
            rank[s][2] = 1 - score_u[s]

        return sorted(rank, key=lambda x: -x[2])

    def predict_per_item(self, user):
        rank = []
        score_i = []
        for item_j in self.uns_items[user]:
            score = 0
            ji = self.training_set['mi'][item_j]
            for item_i in self.training_set['feedback'][user]:
                jj = self.training_set['mi'][item_i]
                score += self.similarity_item[ji, jj]
            rank.append([user, item_j, score])
            score_i.append(score)

        score_i = np.array(score_i)
        score_i = (score_i - min(score_i)) / (max(score_i) - min(score_i))

        for s, sc in enumerate(score_i):
            rank[s][2] = 1 - score_i[s]

        return sorted(rank, key=lambda x: -x[2])

    def recommender(self, pred_by='user'):
        for n, k in enumerate(self.list_row):
            cols = self.density[n].argsort()
            cols = np.array(cols).ravel()[::-1]

            for u_idx in k:
                user = self.training_set['map_user'][u_idx]
                unseen_items = set()
                for l in cols:
                    if self.density[n, l] != 0 and self.density[n, l] != 1:
                        for i_idx in self.list_col[l]:
                            item = self.training_set['map_item'][i_idx]
                            if self.training_set['feedback'][user].get(item, -1) == -1:
                                unseen_items.add(item)

                    if len(unseen_items) > self.number_pred_items:
                        break

                self.uns_items[user] = unseen_items

        if pred_by == 'user':
            self.similarity_user = np.float32(squareform(pdist(self.training_set['matrix'], 'cosine')))
        elif pred_by == 'hybrid':
            self.similarity_user = np.float32(squareform(pdist(self.training_set['matrix'], 'cosine')))
            self.similarity_item = np.float32(squareform(pdist(self.training_set['matrix'].T, 'cosine')))
        else:
            self.similarity_item = np.float32(squareform(pdist(self.training_set['matrix'].T, 'cosine')))

        for user in self.training_set['users']:
            if pred_by == 'hybrid':
                final_rank_dict = defaultdict()
                final_rank_list = []
                ru = self.predict_per_user(user)[:10]
                ri = self.predict_per_item(user)[:10]

                for sample in ru:
                    final_rank_dict[sample[1]] = sample[2]

                for sample in ri:
                    final_rank_dict[sample[1]] = (final_rank_dict.get(sample[1], 0) + sample[2]) / \
                                                 (1 if final_rank_dict.get(sample[1], 0) == 0 else 2)

                for item in final_rank_dict:
                    final_rank_list.append((user, item, final_rank_dict[item]))

                self.predictions += sorted(final_rank_list, key=lambda x: -x[2])[:10]

            elif pred_by == 'user':
                self.predictions += self.predict_per_user(user)[:10]
            else:
                self.predictions += self.predict_per_item(user)[:10]

        # self.predictions = ri

        if self.prediction_file is not None:
            self.write_predictions()

    def write_predictions(self):
        with open(self.prediction_file, 'w') as fw:
            for sample in self.predictions:
                fw.write("%d\t%d\t%f\n" % (sample[0], sample[1], sample[2]))

    def execute(self, header=False):
        if header:
            print("Final entropy::", self.train_model())
            print("K rows:: ", len(self.list_row), "and L columns:: ", len(self.list_col))
            print("Number of bi-groups:: ", len(self.list_row) * len(self.list_col))
            print("Number of bi-groups needing recommendations:: ", self.density[np.logical_and(
                self.density != 1, self.density != 0)].size)
        else:
            self.train_model()
